{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import lightgbm as lgb \n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import accuracy_score,recall_score\n",
    "from sklearn.feature_selection import SelectKBest,SelectPercentile,SelectFromModel,chi2,f_classif,mutual_info_classif,RFE\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "from sklearn.svm import SVC,LinearSVC,LinearSVR,SVR\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from keras.layers import Input, Dense, LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2005-05-01'\n",
    "end_date = '2018-05-31'\n",
    "\n",
    "test_start_date = '2018-06-01'\n",
    "test_end_date = '2020-02-28'\n",
    "start_year = start_date[:4]\n",
    "end_year = end_date[:4]\n",
    "\n",
    "select_index = '000300.XSHG'\n",
    "next_n = 3 #data_y延期n月\n",
    "\n",
    "start_quarter = '2015-05'\n",
    "start_month = '2015-05'\n",
    "\n",
    "seq_len = 5 #lstm back num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "res =  pd.read_csv(\"data/沪深300月度宏观经济数据1.csv\",header=0,index_col=0,sep='\\s+',parse_dates=[1])\n",
    "res_a = pd.read_csv(\"data/沪深300月度宏观经济数据2.csv\",header=0,index_col=0,sep='\\s+',parse_dates=[1])\n",
    "price = pd.read_csv(\"data/沪深300每日交易close价格.csv\",header=0,index_col=0,sep='\\s+',parse_dates=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.index = res['stat_month']\n",
    "res_a.index = res_a['stat_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_macro_data(ml,sd, ed):\n",
    "    res_ = res.loc[ml[0]:ml[-1]]\n",
    "    res_a_ = res_a.loc[ml[0]:ml[-1]]\n",
    "    price_ = price.loc[sd:ed]\n",
    "    return [res_, res_a_, price_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year_list(start_year,end_year):\n",
    "    sy = int(start_year)\n",
    "    ey = int(end_year)\n",
    "    l = []\n",
    "    for i in range(sy,ey+1):\n",
    "        l.append(str(i))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quarter_list(start_year,end_year):\n",
    "    sy = int(start_year)\n",
    "    ey = int(end_year)\n",
    "    l = []\n",
    "    for y in range(sy,ey+1):\n",
    "        for q in range(3,13,3):\n",
    "            if q < 10:\n",
    "                s = str(y) + '-' + '0' + str(q)\n",
    "                l.append(s)\n",
    "            else:\n",
    "                s = str(y) + '-' +  str(q)\n",
    "                l.append(s)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month_list(start_date,end_date):\n",
    "    sy = int(start_date[:4])\n",
    "    ey = int(end_date[:4])\n",
    "    sm = int(start_date[5:7])\n",
    "    em = int(end_date[5:7])\n",
    "    l = []\n",
    "    for y in range(sy,ey+1):\n",
    "        if y == sy:\n",
    "            for i in range(sm,13):\n",
    "                if i< 10:\n",
    "                    s = str(y) + '-' + '0' + str(i)\n",
    "                    l.append(s)\n",
    "                else:\n",
    "                    s = str(y) + '-' +  str(i)      \n",
    "                    l.append(s)\n",
    "               \n",
    "        elif y == ey:\n",
    "            for i in range(1,em+1):\n",
    "                if i < 10:\n",
    "                    s = str(y) + '-' + '0' + str(i)\n",
    "                    l.append(s)\n",
    "                else:\n",
    "                    s = str(y) + '-' +  str(i)\n",
    "                    l.append(s)\n",
    "        else:           \n",
    "            for i in range(1,13):\n",
    "                if i < 10:\n",
    "                    s = str(y) + '-' + '0' + str(i)\n",
    "                    l.append(s)\n",
    "                else:\n",
    "                    s = str(y) + '-' +  str(i)\n",
    "                    l.append(s)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取月度价格数据\n",
    "def get_price_month_data(price_day,n='mean',mean_num=1):\n",
    "    '''\n",
    "    :param price: 输入价格数据，index为datetime类型\n",
    "    :param type: 计算方式，’mean‘取月平均值，若为int,则从第几个交易日开始计算均值，长度为mean_num，\n",
    "    :param mean_num: 计算均值的长度\n",
    "    :return:\n",
    "    '''\n",
    "    ind = list(price_day.index)\n",
    "    s_ind = [datetime.datetime.strftime(i, '%Y%m%d') for i in ind]\n",
    "    price_day.index = s_ind\n",
    "    num_ind = [int(i) for i in s_ind]\n",
    "    cut_ind = [int(i / 100) for i in num_ind]\n",
    "    cut_s_ind = [(str(i)[:4] + '-' + str(i)[4:]) for i in cut_ind]\n",
    "    price_day['stat_date'] = cut_s_ind\n",
    "    if n == 'mean':\n",
    "        res = price_day.groupby(by=['stat_date']).mean()\n",
    "    else:\n",
    "        ind_sig = list(set(price['stat_date'].values))\n",
    "        index_list = []\n",
    "        mean_list = []\n",
    "        for ind in ind_sig:\n",
    "            df = price[price['stat_date']==ind]\n",
    "            sel_df = df.iloc[n-1:n+mean_num-1,0]\n",
    "            index = list(sel_df.index)\n",
    "            if len(index) == 0:\n",
    "                continue\n",
    "            index = index[0]\n",
    "            index_list.append(index)\n",
    "            mean_df = sel_df.mean()\n",
    "            mean_list.append(mean_df)\n",
    "        res_df = pd.DataFrame(mean_list,index=index_list,columns=['month_price'])\n",
    "        res = res_df.sort_index()\n",
    "        res_index = list(res.index)\n",
    "        ind_s_cut = [i[:4] + '-' + i[4:6] for i in res_index]\n",
    "        res.index = ind_s_cut\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pure_values(data):\n",
    "    '''\n",
    "    获取纯净数值，将DataFrame中的非数值项剔除，例如‘code’项（str）\n",
    "    input:\n",
    "    data:pd.DataFrame,index为股票代码\n",
    "    putput:\n",
    "    DataFrame：只含数值项\n",
    "    '''\n",
    "    columns = list(data.columns)\n",
    "    for column in columns:\n",
    "        if not(isinstance(data[column][0],int) or isinstance(data[column][0],float)):\n",
    "            data = data.drop([column],axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def winsorize_and_standarlize(data, qrange=[0.05, 0.95], axis=0):\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        if axis == 0:\n",
    "            q_down = data.quantile(qrange[0])\n",
    "            q_up = data.quantile(qrange[1])\n",
    "            col = data.columns\n",
    "            for n in col:\n",
    "                data[n][data[n] > q_up[n]] = q_up[n]\n",
    "                data[n][data[n] < q_down[n]] = q_down[n]\n",
    "            data = (data - data.mean()) / data.std()\n",
    "            data = data.fillna(0)\n",
    "        else:\n",
    "            data = data.stack()\n",
    "            data = data.unstack(0)\n",
    "            q_down = data.quantile(qrange[0])\n",
    "            q_up = data.quantile(qrange[1])\n",
    "            col = data.columns\n",
    "            for n in col:\n",
    "                data[n][data[n] > q_up[n]] = q_up[n]\n",
    "                data[n][data[n] < q_down[n]] = q_down[n]\n",
    "            data = (data - data.mean()) / data.std()\n",
    "            data = data.stack().unstack(0)\n",
    "            data = data.fillna(0)\n",
    "\n",
    "    elif isinstance(data, pd.Series):\n",
    "        q_down = data.quantile(qrange[0])\n",
    "        q_up = data.quantile(qrange[1])\n",
    "        data[data > q_up] = q_up\n",
    "        data[data < q_down] = q_down\n",
    "        data = (data - data.mean()) / data.std()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_profit_class(data):\n",
    "    '''\n",
    "    对数据进行分类标记\n",
    "    '''\n",
    "    data_diff = data.diff(next_n)\n",
    "    data_diff[data_diff > 0] = 1\n",
    "    data_diff[data_diff < 0] = 0\n",
    "    #data_diff[data_diff == 2] = -1\n",
    "    #data_diff[data_diff == -2] = 1\n",
    "    return data_diff\n",
    "\n",
    "def get_final_data(input_x,input_y):\n",
    "    input_y = input_y.shift(-next_n).to_frame().fillna(method='ffill')\n",
    "    data_m = pd.merge(input_x,input_y,left_index=True,right_index=True,how='right')\n",
    "    columns_m = data_m.columns\n",
    "    data_x = data_m[columns_m[:-1]]\n",
    "    data_y = data_m[columns_m[-1]]\n",
    "    return data_x,data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelection():\n",
    "    '''\n",
    "    特征选择：\n",
    "    identify_collinear：基于相关系数，删除小于correlation_threshold的特征\n",
    "    identify_importance_lgbm：基于LightGBM算法，得到feature_importance,选择和大于p_importance的特征\n",
    "    filter_select:单变量选择，指定k,selectKBest基于method提供的算法选择前k个特征，selectPercentile选择前p百分百的特征\n",
    "    wrapper_select:RFE，基于estimator递归特征消除，保留n_feature_to_select个特征\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.filter_supports = None #bool型，特征是否被选中\n",
    "        self.wrapper_supports = None\n",
    "        self.embedded_supports = None\n",
    "        self.lgbm_columns = None  #选择的特征\n",
    "        self.filter_columns = None\n",
    "        self.wrapper_columns = None\n",
    "        self.embedded_columns = None\n",
    "        self.record_collinear = None #自相关矩阵大于门限值\n",
    "        \n",
    "    def identify_collinear(self, data, correlation_threshold):\n",
    "\n",
    "        columns = data.columns\n",
    "        self.correlation_threshold = correlation_threshold\n",
    "\n",
    "        # Calculate the correlations between every column\n",
    "        corr_matrix = data.corr()\n",
    "        \n",
    "        self.corr_matrix = corr_matrix\n",
    "    \n",
    "        # Extract the upper triangle of the correlation matrix\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n",
    "        # Select the features with correlations above the threshold\n",
    "        # Need to use the absolute value\n",
    "        to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]\n",
    "        obtain_columns = [column for column in columns if column not in to_drop]\n",
    "        self.columns = obtain_columns\n",
    "        # Dataframe to hold correlated pairs\n",
    "        record_collinear = pd.DataFrame(columns = ['drop_feature', 'corr_feature', 'corr_value'])\n",
    "\n",
    "        # Iterate through the columns to drop\n",
    "        for column in to_drop:\n",
    "\n",
    "            # Find the correlated features\n",
    "            corr_features = list(upper.index[upper[column].abs() > correlation_threshold])\n",
    "\n",
    "            # Find the correlated values\n",
    "            corr_values = list(upper[column][upper[column].abs() > correlation_threshold])\n",
    "            drop_features = [column for _ in range(len(corr_features))]    \n",
    "\n",
    "            # Record the information (need a temp df for now)\n",
    "            temp_df = pd.DataFrame.from_dict({'drop_feature': drop_features,\n",
    "                                             'corr_feature': corr_features,\n",
    "                                             'corr_value': corr_values})\n",
    "\n",
    "            # Add to dataframe\n",
    "            record_collinear = record_collinear.append(temp_df, ignore_index = True)\n",
    "\n",
    "        self.record_collinear = record_collinear\n",
    "        return data[obtain_columns]\n",
    "     \n",
    "        \n",
    "    def identify_importance_lgbm(self, features, labels,p_importance=0.8, eval_metric='auc', task='classification', \n",
    "                                 n_iterations=10, early_stopping = True):\n",
    "        \n",
    "        # One hot encoding\n",
    "        data = features\n",
    "        features = pd.get_dummies(features)\n",
    "\n",
    "        # Extract feature names\n",
    "        feature_names = list(features.columns)\n",
    "\n",
    "        # Convert to np array\n",
    "        features = np.array(features)\n",
    "        labels = np.array(labels).reshape((-1, ))\n",
    "\n",
    "        # Empty array for feature importances\n",
    "        feature_importance_values = np.zeros(len(feature_names))\n",
    "        \n",
    "        #print('Training Gradient Boosting Model\\n')\n",
    "        \n",
    "        # Iterate through each fold\n",
    "        for _ in range(n_iterations):\n",
    "\n",
    "            if task == 'classification':\n",
    "                model = lgb.LGBMClassifier(n_estimators=100, learning_rate = 0.05, verbose = 1)\n",
    "\n",
    "            elif task == 'regression':\n",
    "                model = lgb.LGBMRegressor(n_estimators=100, learning_rate = 0.05, verbose = 1)\n",
    "\n",
    "            else:\n",
    "                raise ValueError('Task must be either \"classification\" or \"regression\"')\n",
    "                \n",
    "            # If training using early stopping need a validation set\n",
    "            if early_stopping:\n",
    "                \n",
    "                train_features, valid_features, train_labels, valid_labels = train_test_split(features, labels, test_size = 0.15)\n",
    "\n",
    "                # Train the model with early stopping\n",
    "                model.fit(train_features, train_labels, eval_metric = eval_metric,\n",
    "                          eval_set = [(valid_features, valid_labels)],\n",
    "                           verbose = -1)\n",
    "                \n",
    "                # Clean up memory\n",
    "                gc.enable()\n",
    "                del train_features, train_labels, valid_features, valid_labels\n",
    "                gc.collect()\n",
    "                \n",
    "            else:\n",
    "                model.fit(features, labels)\n",
    "\n",
    "            # Record the feature importances\n",
    "            feature_importance_values += model.feature_importances_ / n_iterations\n",
    "\n",
    "        feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "\n",
    "        # Sort features according to importance\n",
    "        feature_importances = feature_importances.sort_values('importance', ascending = False).reset_index(drop = True)\n",
    "\n",
    "        # Normalize the feature importances to add up to one\n",
    "        feature_importances['normalized_importance'] = feature_importances['importance'] / feature_importances['importance'].sum()\n",
    "        feature_importances['cumulative_importance'] = np.cumsum(feature_importances['normalized_importance'])\n",
    "        select_df = feature_importances[feature_importances['cumulative_importance']<=p_importance]\n",
    "        select_columns = select_df['feature']\n",
    "        self.lgbm_columns = list(select_columns.values)\n",
    "        res = data[self.columns]\n",
    "        return res\n",
    "        \n",
    "    def filter_select(self, data_x, data_y, k=None, p=50,method=f_classif):\n",
    "        columns = data_x.columns\n",
    "        if k != None:\n",
    "            model = SelectKBest(method,k)\n",
    "            res = model.fit_transform(data_x,data_y)\n",
    "            supports = model.get_support()\n",
    "        else:\n",
    "            model = SelectPercentile(method,p)\n",
    "            res = model.fit_transform(data_x,data_y)\n",
    "            supports = model.get_support()\n",
    "        self.filter_support_ = supports\n",
    "        self.filter_columns = columns[supports]\n",
    "        return res\n",
    "    \n",
    "    def wrapper_select(self,data_x,data_y,n,estimator):\n",
    "        columns = data_x.columns\n",
    "        model = RFE(estimator=estimator,n_features_to_select=n)\n",
    "        res = model.fit_transform(data_x,data_y)\n",
    "        supports = model.get_support() #标识被选择的特征在原数据中的位置\n",
    "        self.wrapper_supports = supports\n",
    "        self.wrapper_columns = columns[supports]\n",
    "        return res\n",
    "    \n",
    "    def embedded_select(self,data_x,data_y,estimator,threshold=None):\n",
    "        columns = data_x.columns\n",
    "        model = SelectFromModel(estimator=estimator,prefit=False,threshold=threshold)\n",
    "        res = model.fit_transform(data_x,data_y)\n",
    "        supports = model.get_support()\n",
    "        self.embedded_supports = supports\n",
    "        self.embedded_columns = columns[supports]\n",
    "        print(\"self.embedded_columns:\",self.embedded_columns)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调参时计算召回率和准确率\n",
    "def lstm_recall(prediction,y,n=1,thr=0.5):\n",
    "    len_p = len(prediction)\n",
    "    l = 0\n",
    "    z = 0\n",
    "    res = []\n",
    "    if n == 1:\n",
    "        for i in range(len_p):\n",
    "            if (prediction[i]>= thr):\n",
    "                l = l+1\n",
    "                if (y[i] ==1):\n",
    "                    z = z+1\n",
    "    elif n==0:\n",
    "         for i in range(len_p):\n",
    "            if (prediction[i]<=thr):\n",
    "                l = l+1\n",
    "                if (y[i] ==0):\n",
    "                    z = z+1\n",
    "    lstm_recall = z/l\n",
    "    return lstm_recall\n",
    "\n",
    "def lstm_accuracy(prediction,y,thr=0.5):\n",
    "    len_p = len(prediction)\n",
    "    l = 0\n",
    "    for i in range(len_p):\n",
    "        if ((prediction[i]>=thr) & (y[i] ==1)) |  ((prediction[i]<thr) & (y[i] ==0)):\n",
    "            l = l + 1\n",
    "            \n",
    "    accuracy = l/len_p\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_output(res,buy_thr=1.5,sell_thr=0.5):\n",
    "    '''\n",
    "    基于lgbm和embedded两组特征选择数据的计算结果确定预测结果\n",
    "    buy_thr:预测股价上升的门限值，默认1.5,\n",
    "    sell_thr:预测股价下降的门限值，默认0.5\n",
    "    大于买入门限标记为1，小于卖出门限标记为-1，中间值认为买入卖出信号不强，选择观望或空仓，卖出信号可用于做空，在无法做空时认为空仓\n",
    "    \n",
    "    '''\n",
    "    length = len(res)\n",
    "    l = []\n",
    "    for i in range(length):\n",
    "        if res[i] > buy_thr:\n",
    "            l.append(1)\n",
    "        elif res[i] < sell_thr:\n",
    "            l.append(-1)\n",
    "        else:\n",
    "            l.append(0)\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_fun_keras(n_input,train_x,train_y,predict_x,prediction_y,seq_len=5):\n",
    "    K.get_session().close()\n",
    "    K.set_session(tf.Session())\n",
    "    K.get_session().run(tf.global_variables_initializer())\n",
    "    INPUT_DIM = n_input\n",
    "    output_dim = 1\n",
    "    batch_size = 8 #每轮训练模型时，样本的数量\n",
    "    epochs = 100 #训练60轮次\n",
    "    hidden_size = 64\n",
    "    lstm_units = 32\n",
    "    inputs = Input(shape=(seq_len, INPUT_DIM))\n",
    "    #drop1 = Dropout(0.3)(inputs)\n",
    "    x = Conv1D(filters = 64, kernel_size = 1, activation = 'relu')(inputs)  #, padding = 'same'\n",
    "    #x = Conv1D(filters=128, kernel_size=5, activation='relu')(output1)#embedded_sequences\n",
    "    x = MaxPooling1D(pool_size = seq_len)(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    #print(x.shape)\n",
    "    lstm_out = Bidirectional(LSTM(lstm_units, activation='relu'), name='bilstm')(x)\n",
    "    #lstm_out = LSTM(lstm_units,activation='relu')(x)\n",
    "    #print(lstm_out.shape)\n",
    "    # ATTENTION PART STARTS HERE\n",
    "    attention_probs = Dense(64, activation='sigmoid', name='attention_vec')(lstm_out)\n",
    "    #attention_mul=layers.merge([stm_out,attention_probs], output_shape],mode='concat',concat_axis=1))\n",
    "    attention_mul =Multiply()([lstm_out, attention_probs])\n",
    "    #attention_mul = merge([lstm_out, attention_probs],output_shape=32, name='attention_mul', mode='mul')\n",
    "    #h1 = Dense(hidden_size, activation='relu')(attention_mul)\n",
    "    output_class = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    #output = Dense(10, activation='sigmoid')(drop2)\n",
    "\n",
    "    model_class = Model(inputs=inputs, outputs=output_class)\n",
    "    #print(model_class.summary())\n",
    "    #model_class.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model_class.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    # simple early stopping\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "    # fit model\n",
    "    history_class = model_class.fit(train_x, train_y,validation_split=0.2, epochs=epochs, batch_size=batch_size, shuffle=False, callbacks=[es])\n",
    "    y_pred_class = model_class.predict(predict_x)\n",
    "    print(\"accuracy:\",lstm_accuracy(y_pred_class,prediction_y,thr=0.5))\n",
    "    return y_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_fun(n_input,train_x,train_y,predict_x,prediction_y,seq_len=5):\n",
    "    #LSTM\n",
    "    lr=0.1\n",
    "    lstm_size = 10  #lstm cell数量，基于数据量调整\n",
    "    epoch_num = 10  #打印次数，和n_batch相乘便是迭代次数\n",
    "    n_batch = 100\n",
    "    lookback = seq_len\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    x = tf.placeholder(tf.float32,[None,lookback,n_input])\n",
    "    y = tf.placeholder(tf.float32,[None,1])\n",
    "\n",
    "    weights = tf.Variable(tf.truncated_normal([lstm_size,1],stddev=0.1))\n",
    "    biases = tf.Variable(tf.constant(0.1,shape=[1]))\n",
    "\n",
    "    def LSTM_net(x,weights,biases):\n",
    "        lstm_cell = tf.contrib.rnn.LSTMCell(lstm_size,name='basic_lstm_cell') #.BasicLSTMCell(lstm_size)\n",
    "        output,final_state = tf.nn.dynamic_rnn(lstm_cell,x,dtype=tf.float32)\n",
    "        results = tf.nn.sigmoid(tf.matmul(final_state[1],weights)+biases)\n",
    "        return results\n",
    "\n",
    "    prediction = LSTM_net(x,weights,biases)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for i in range(epoch_num):\n",
    "            for j in range(n_batch):\n",
    "                sess.run(train_step,feed_dict={x:train_x,y:train_y})\n",
    "                train_loss = sess.run(loss,feed_dict={x:train_x,y:train_y})\n",
    "            print('train loss is'+ str(train_loss))\n",
    "        prediction_res = sess.run(prediction,feed_dict={x:predict_x})\n",
    "    print(\"accuracy:\",lstm_accuracy(prediction_res,prediction_y,thr=0.5))\n",
    "    return prediction_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_time_list(start_date,test_start_date,test_end_date):\n",
    "    '''\n",
    "    使用机器学习算法计算时的时间数据\n",
    "    input:\n",
    "    start_date:取宏观数据的开始时间\n",
    "    test_start_date:预测的开始时间，也就是取宏观数据的截止时间\n",
    "    test_end_date:预测的截止时间，对期间的每一个月都要预测，每一个月都要重跑一遍预测函数，每个月的开始使用上个月28号的价格数据作为截止价格时间\n",
    "    output:\n",
    "    m_l：每一次预测时的月份列表，在此列表内跑预测函数，\n",
    "    start_date,取宏观数据和价格数据的开始时间，由全局变量定义\n",
    "    ed:取价格数据截止时间列表，顺序和m_l对应\n",
    "    '''\n",
    "    test_month = get_month_list(test_start_date,test_end_date)\n",
    "    m_l_pre = []\n",
    "    ed = []\n",
    "    for m in test_month:\n",
    "        ml = get_month_list(start_date,m)\n",
    "        m_l_pre.append(ml)\n",
    "        if m == test_month[-1]:\n",
    "            ed.append(test_end_date)\n",
    "        else:\n",
    "            ed.append(m+'-'+'28')\n",
    "    return m_l_pre,start_date,ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_test,sd_test,ed_test = get_test_time_list(start_date,test_start_date,test_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monthly_fun(month_list, start_date, end_date):\n",
    "    macro_data_o = get_macro_data(month_list,start_date, end_date)\n",
    "    res = macro_data_o[0]\n",
    "    res_a = macro_data_o[1]\n",
    "    price = macro_data_o[2]\n",
    "    price_month = get_price_month_data(price)\n",
    "    res.index = res['stat_month']\n",
    "    res_a.index = res['stat_month']\n",
    "\n",
    "    sel_data = pd.merge(res_a, price_month, left_index=True, right_index=True).dropna()  # 使用全部宏观数据\n",
    "    data_pure = get_pure_values(sel_data)\n",
    "    columns = data_pure.columns\n",
    "    data_x_start = data_pure[columns[:-1]]\n",
    "    data_y_o = data_pure[columns[-1]]\n",
    "    data_x_o = winsorize_and_standarlize(data_x_start)\n",
    "    data_y_class_o = get_profit_class(data_y_o)\n",
    "    data_x, data_y_class = get_final_data(data_x_o, data_y_class_o)\n",
    "    _, data_y = get_final_data(data_x_o, data_y_o)\n",
    "    # 特征选择\n",
    "    #print(\"特征选择\")\n",
    "    f = FeatureSelection()\n",
    "    #print(\"去除一些共线性特征\")\n",
    "    n_collinear = f.identify_collinear(data_x, correlation_threshold=0.8) #去除一些共线性特征\n",
    "    #print(\"lightgbm 重要特征\")\n",
    "    lgbm_res = f.identify_importance_lgbm(n_collinear, data_y_class, p_importance=0.9)\n",
    "\n",
    "    #estimator = LinearSVC()\n",
    "    #wrapper_res = f.wrapper_select(data_x=n_collinear, data_y=data_y_class, n=5, estimator=estimator)\n",
    "\n",
    "    #est = LinearSVC(C=0.01, penalty='l1', dual=False)\n",
    "    #print(\"random forest 重要特征\")\n",
    "    est1 = RandomForestClassifier(verbose=1)\n",
    "    embedded_res = f.embedded_select(data_x=n_collinear, data_y=data_y_class, estimator=est1)\n",
    "    # LSTM数据准备\n",
    "    #print(\"数据准备\")\n",
    "    lgbm_n_input = len(lgbm_res.columns)\n",
    "    print(\"lgbm_res.columns:\",lgbm_res.columns)\n",
    "    lgbm_x_a = np.array(lgbm_res)\n",
    "    lgbm_x_o = [lgbm_x_a[i: i + seq_len, :] for i in range(lgbm_res.shape[0] - seq_len)]\n",
    "    lgbm_x_l = len(lgbm_x_o)\n",
    "    lgbm_x_array = np.reshape(lgbm_x_o, [lgbm_x_l, seq_len, lgbm_n_input])\n",
    "    lgbm_x = lgbm_x_array[:-next_n]\n",
    "    lgbm_x_prediction = lgbm_x_array[-next_n:]\n",
    "    lgbm_y_o = np.array([data_y_class[i + seq_len] for i in range(len(data_y_class) - seq_len)])\n",
    "    lgbm_y_array = np.reshape(lgbm_y_o, [lgbm_y_o.shape[0], 1])\n",
    "    lgbm_y = lgbm_y_array[:-next_n]\n",
    "    lgbm_y_prediction = lgbm_y_array[-next_n:]\n",
    "    # print(len(lgbm_y))\n",
    "\n",
    "    embedded_n_input = np.shape(embedded_res)[1]\n",
    "    embedded_x_a = np.array(embedded_res)\n",
    "    embedded_x_o = [embedded_x_a[i: i + seq_len, :] for i in range(embedded_res.shape[0] - seq_len)]\n",
    "    embedded_x_l = len(embedded_x_o)\n",
    "    embedded_x_array = np.reshape(embedded_x_o, [embedded_x_l, seq_len, embedded_n_input])\n",
    "    embedded_x = embedded_x_array[:-next_n]\n",
    "    embedded_x_predition = embedded_x_array[-next_n:]\n",
    "    embedded_y_o = np.array([data_y_class[i + seq_len] for i in range(len(data_y_class) - seq_len)])\n",
    "    embedded_y_array = np.reshape(embedded_y_o, [embedded_y_o.shape[0], 1])\n",
    "    embedded_y = embedded_y_array[:-next_n]\n",
    "\n",
    "    # 获取训练数据和测试数据\n",
    "    # lgbm_train_x,lgbm_test_x,lgbm_train_y,lgbm_test_y = train_test_split(lgbm_x,lgbm_y,test_size=0.3,random_state=0)\n",
    "    # embedded_train_x,embedded_test_x,embedded_train_y,embedded_test_y = train_test_split(embedded_x,embedded_y,test_size=0.3,random_state=0)\n",
    "    #print(\"lightgbm 特征训练lstm\")\n",
    "    lgbm_pre_res = LSTM_fun(lgbm_n_input, lgbm_x, lgbm_y, lgbm_x_prediction, lgbm_y_prediction,seq_len=seq_len)\n",
    "    print(\"lgbm_pre_res: \",lgbm_pre_res)\n",
    "    #print(\"random forest 特征训练lstm\")\n",
    "    embedded_pre_res = LSTM_fun(embedded_n_input, embedded_x, embedded_y, embedded_x_predition, lgbm_y_prediction,seq_len=seq_len)\n",
    "    print(\"embedded_pre_res: \",embedded_pre_res)\n",
    "    # print(lgbm_pre_res)\n",
    "    # print(embedded_pre_res)\n",
    "    pre_res = lgbm_pre_res + embedded_pre_res  # 将两组数据的的预测结果相加\n",
    "    #signal = res_output(pre_res)\n",
    "    # print(signal)\n",
    "    print(\"predict result:\", pre_res)\n",
    "    return pre_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pre_signal(ml,sd,ed):\n",
    "    '''\n",
    "    在时间序列下计算最后三个月的预测值，以此作为决策信号\n",
    "    '''\n",
    "    start_clock = time.clock()\n",
    "    length = len(ml)\n",
    "    l = []\n",
    "    for i in range(length):\n",
    "        month_l = ml[i]\n",
    "        end_d = ed[i]\n",
    "        print(\"predict month:\",end_d)\n",
    "        pre = monthly_fun(month_l,sd,end_d)\n",
    "        l.append(pre)\n",
    "    end_clock = time.clock()\n",
    "    clofk_diff = (end_clock - start_clock)/60\n",
    "    print('time cost:%0.3f'%clofk_diff)\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict month: 2018-06-28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.embedded_columns: Index(['retail_sin', 'retail_sin_yoy', 'primary_yoy', 'centre_project_yoy',\n",
      "       'enterprise_value_acc_diff', 'total_interest_ratio_acc_diff', 'id',\n",
      "       'loss_enterprise_ratio_acc', 'finished_product_ratio_acc'],\n",
      "      dtype='object')\n",
      "lgbm_res.columns: Index(['retail_sin', 'retail_sin_yoy', 'invest', 'primary_yoy',\n",
      "       'centre_project_yoy', 'pmi', 'growth_acc_diff',\n",
      "       'enterprise_value_acc_diff', 'loss_enterprise_ratio_acc_diff',\n",
      "       'total_interest_ratio_acc_diff', 'id', 'enterprise_value_acc',\n",
      "       'loss_enterprise_value_acc', 'loss_enterprise_ratio_acc',\n",
      "       'finished_product_ratio_acc', 'main_business_tax_value_acc',\n",
      "       'main_business_tax_ratio_acc', 'enterprise_total_loss_ratio_acc'],\n",
      "      dtype='object')\n",
      "train loss is0.18679881\n",
      "train loss is0.12955938\n",
      "train loss is0.09842617\n",
      "train loss is0.07510271\n",
      "train loss is0.05898084\n",
      "train loss is0.046202615\n",
      "train loss is0.034892023\n",
      "train loss is0.024432175\n",
      "train loss is0.018270629\n",
      "train loss is0.013892202\n",
      "accuracy: 1.0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-5040249a1c9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpre_res_l\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_pre_signal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mml_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msd_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0med_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-37-1997fd4232c0>\u001b[0m in \u001b[0;36mget_pre_signal\u001b[1;34m(ml, sd, ed)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mend_d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0med\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"predict month:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mend_d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mpre\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmonthly_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonth_l\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mend_d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mend_clock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-2e471f0fa238>\u001b[0m in \u001b[0;36mmonthly_fun\u001b[1;34m(month_list, start_date, end_date)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;31m# embedded_train_x,embedded_test_x,embedded_train_y,embedded_test_y = train_test_split(embedded_x,embedded_y,test_size=0.3,random_state=0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;31m#print(\"lightgbm 特征训练lstm\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[0mlgbm_pre_res\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlgbm_n_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlgbm_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlgbm_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlgbm_x_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlgbm_y_prediction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"lgbm_pre_res: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlgbm_pre_res\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;31m#print(\"random forest 特征训练lstm\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-387a150e0164>\u001b[0m in \u001b[0;36mLSTM_fun\u001b[1;34m(n_input, train_x, train_y, predict_x, prediction_y, seq_len)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mprediction_res\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mpredict_x\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"accuracy:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlstm_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction_res\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprediction_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mthr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"recall:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlstm_recall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction_res\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprediction_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mthr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mprediction_res\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-aa356fae01ba>\u001b[0m in \u001b[0;36mlstm_recall\u001b[1;34m(prediction, y, n, thr)\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mlstm_recall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlstm_recall\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "pre_res_l = get_pre_signal(ml_test,sd_test,ed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_res_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[res_output(i) for i in pre_res_l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#只适应与n=3的情况\n",
    "def get_buy_sell_signal(pre_res):\n",
    "    '''\n",
    "    由于预测的是未来三个月（n=3）的的情况，预测有重合部分，将重合部分取平均，基于均值做买卖决策，提高精度\n",
    "    此函数只适应与n=3的情况\n",
    "    '''\n",
    "    length = len(pre_res)\n",
    "    l = []\n",
    "    for i in range(length):\n",
    "        if i == 0:\n",
    "            l.append(pre_res[0][0])\n",
    "        elif i == 1:\n",
    "            s = (pre_res[0][1]+pre_res[1][0])/2\n",
    "            l.append(s)\n",
    "        elif i == length-1:\n",
    "            l.append((pre_res[i][0]+pre_res[i-1][1]+pre_res[i-2][2])/3)\n",
    "            l.append((pre_res[i][1]+pre_res[i-1][2])/2)\n",
    "            l.append(pre_res[i][2])\n",
    "        else:\n",
    "            t = (pre_res[i][0]+pre_res[i-1][1]+pre_res[i-2][2])/3\n",
    "            l.append(t)\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_signal = get_buy_sell_signal([res_output(i) for i in pre_res_l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_buy_month_list(signal,test_start_date,test_end_date):\n",
    "    test_month = get_month_list(test_start_date,test_end_date)\n",
    "    length = len(signal)\n",
    "    dic = {}\n",
    "    for i in range(next_n,length+1):\n",
    "        l = []\n",
    "        for j in range(next_n):\n",
    "            l.append(signal[i-(next_n-j)])\n",
    "        dic[test_month[i-next_n]] = l\n",
    "    return dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = get_buy_month_list(bs_signal,test_start_date,test_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month_buy_signal(dic,test_start_date,test_end_date):\n",
    "    '''\n",
    "    获取每月买卖信号，卖出信号为[0,0,0]、[1,0,0]、[1,1,0]，其余为卖出信号,001、010、011、101、111为买入信号，信号分析没有考虑做空\n",
    "    input:\n",
    "    dic: dic,key为月份，value为对应的信号\n",
    "    ''' \n",
    "    test_month = get_month_list(test_start_date,test_end_date)\n",
    "    dic_month_signal = OrderedDict()\n",
    "    for m in test_month:\n",
    "        l = dic[m]\n",
    "        if (l[0]<0.5) & (l[1]<0.5) & (l[2]<0.5):\n",
    "            dic_month_signal[m] = 0\n",
    "        elif (l[0]>0.5) & (l[1]<0.5) & (l[2]<0.5):\n",
    "            dic_month_signal[m] = 0\n",
    "        elif (l[0]>0.5) & (l[1]>0.5) & (l[2]<0.5):\n",
    "            dic_month_signal[m] = 0\n",
    "        else:\n",
    "            dic_month_signal[m] = 1\n",
    "    v = list(dic_month_signal.values())\n",
    "    df = pd.DataFrame(v,index=dic_month_signal.keys(),columns=['signal'])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_signal = get_month_buy_signal(dic,test_start_date,test_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month_profit(stocks,start_date,end_date):\n",
    "    '''\n",
    "    获取月收益率数据，数据为本月相对于上月的增长率，计算时用每月最后MONTH_MEAN_DAY_NUM天的均值\n",
    "    input:\n",
    "    data:dataframe,index为股票代码，values为因子值\n",
    "    start_date:str, 初始日期\n",
    "    end_date:str,终止日期\n",
    "    output:\n",
    "    month_profit_df: Dataframe,columns为每月第一天的收盘价\n",
    "    \n",
    "    '''\n",
    "    start_year = int(start_date[:4])\n",
    "    end_year = int(end_date[:4])\n",
    "    start_month = int(start_date[5:7])\n",
    "    end_month = int(end_date[5:7])\n",
    "    len_month = (end_year - start_year)*12 + (end_month - start_month) + 2\n",
    "    price_list = []\n",
    "    month = start_month-1\n",
    "    year = start_year\n",
    "    for i in range(len_month):\n",
    "        year = year + (month + 1) // 13\n",
    "        month = month % 12 + 1\n",
    "        date_s = str(year)+'-'+str(month)+'-'+'01'\n",
    "        date_e = str(year)+'-'+str(month)+'-'+'20'\n",
    "        price_close = price.loc[date_s:date_e]['close'][0:1]\n",
    "        #print(price_close)\n",
    "        #price = get_price(stocks,fields=['close'],count=1,end_date=date)['close']\n",
    "        price_list.append(price_close)\n",
    "    month_profit = pd.concat(price_list,axis=0)\n",
    "    v = list(month_profit.values)\n",
    "    month_profit_df = pd.DataFrame(v,index=month_profit.index,columns=['profit'])\n",
    "    return month_profit_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_profit = get_month_profit(select_index,test_start_date,test_end_date)\n",
    "month_profit_pct = month_profit.pct_change(1,axis=0).dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strategy_profit(month_signal,month_profit_pct):\n",
    "    length_signal = len(month_signal)\n",
    "    length_pct = len(month_profit_pct)\n",
    "    if length_signal != length_pct:\n",
    "        print('input references must have same length')\n",
    "    month_profit_pct_shift = month_profit_pct['profit'].shift(-1)\n",
    "    month_signal['profit'] = month_profit_pct_shift.values\n",
    "\n",
    "    month_signal = month_signal.dropna()\n",
    "    month_signal['profit'][month_signal['signal']==0] = 0\n",
    "    month_signal['selct_profit'] = month_signal['profit']\n",
    "    month_signal['cumprod_profit'] = (month_signal['selct_profit']+1).cumprod()\n",
    "    month_signal['cumsum_profit'] = month_signal['selct_profit'].cumsum()\n",
    "    return month_signal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_profit = get_strategy_profit(month_signal,month_profit_pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_cumprod = strategy_profit['cumprod_profit']\n",
    "p_hs300 = price.loc[test_start_date:test_end_date]['close']\n",
    "#p_hs300 = get_price(select_index,start_date=test_start_date,end_date=test_end_date,fields=['close'])\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax0 = fig.add_subplot(2,1,1)\n",
    "ax1 = fig.add_subplot(2,1,2)\n",
    "ax0.plot(sp_cumprod)\n",
    "ax1.plot(p_hs300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
